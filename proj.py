import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import streamlit as st
import io
import base64

# Configuration de la page
st.set_page_config(
    page_title="Outil d'Analyse ACP",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Ajout de CSS personnalis√© pour le style
st.markdown("""
<style>
    .main {
        padding: 2rem;
    }
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    h1 {
        color: #1E88E5;
    }
    h2 {
        color: #388E3C;
        margin-top: 2rem;
    }
    .interpretation {
        background-color: #f8f9fa;
        border-left: 4px solid #1E88E5;
        padding: 1rem;
        margin: 1rem 0;
        color: #333;
    }
    .interpretation h3 {
        color: #2C3E50;
        margin-top: 1rem;
        margin-bottom: 0.75rem;
    }
    .interpretation ul, .interpretation ol {
        margin-left: 1.5rem;
        color: #333;
    }
</style>
""", unsafe_allow_html=True)

# Titre et introduction
st.title("Analyse en Composantes Principales (ACP) avec Interpr√©tation Automatis√©e")
st.markdown("""
Cette application effectue une Analyse en Composantes Principales (ACP) sur vos donn√©es et fournit :
1. Un graphique d'√©boulis (Scree Plot) montrant la variance expliqu√©e par chaque composante principale
2. Un Biplot montrant la projection des individus et des variables sur CP1 et CP2
3. Une interpr√©tation automatis√©e des r√©sultats de l'ACP

T√©l√©chargez votre fichier CSV pour commencer.
""")

# Barre lat√©rale pour le t√©l√©chargement de fichiers et les param√®tres
with st.sidebar:
    st.header("Donn√©es d'entr√©e")
    uploaded_file = st.file_uploader("T√©l√©charger un fichier CSV", type=["csv"])
    
    st.header("Param√®tres de l'ACP")
    scale_data = st.checkbox("Standardiser les donn√©es (recommand√©)", value=True)
    n_components = st.slider("Nombre maximal de composantes √† calculer", min_value=2, max_value=10, value=5)
    
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        st.header("S√©lection des variables")
        
        # Affichage d'un √©chantillon des donn√©es
        st.subheader("√âchantillon de donn√©es")
        st.dataframe(df.head(5))
        
        # S√©lection de la variable cible (optionnel)
        all_columns = df.columns.tolist()
        color_by = st.selectbox("Colorer les points par (optionnel)", ["Aucun"] + all_columns)
        
        # S√©lection des colonnes pour l'ACP
        st.subheader("S√©lectionner les colonnes pour l'analyse")
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        selected_columns = st.multiselect("S√©lectionner les colonnes pour l'analyse", 
                                        numeric_columns, 
                                        default=numeric_columns[:min(5, len(numeric_columns))])

# Fonction d'interpr√©tation IA
def interpret_pca_results(pca, feature_names, loadings, explained_variance_ratio, df_pca):
    """G√©n√©rer une interpr√©tation des r√©sultats de l'ACP"""
    
    # Interpr√©tation de la variance expliqu√©e
    var_interpretation = f"""### Analyse de la variance expliqu√©e
    
La premi√®re composante principale (CP1) explique {explained_variance_ratio[0]*100:.2f}% de la variance totale des donn√©es.
La deuxi√®me composante principale (CP2) explique {explained_variance_ratio[1]*100:.2f}% suppl√©mentaires de la variance.

Ensemble, CP1 et CP2 capturent {(explained_variance_ratio[0] + explained_variance_ratio[1])*100:.2f}% de l'information totale dans le jeu de donn√©es.
"""

    # Recherche d'un coude dans le graphique d'√©boulis
    diffs = np.diff(explained_variance_ratio)
    elbow_detected = False
    elbow_component = 0
    
    for i in range(len(diffs)):
        if i > 0 and diffs[i-1] > 0.1 and diffs[i] < 0.05:
            elbow_detected = True
            elbow_component = i + 1
            break
    
    if elbow_detected:
        var_interpretation += f"\nUn coude appara√Æt √† la composante {elbow_component}, sugg√©rant que les {elbow_component} premi√®res composantes peuvent √™tre suffisantes pour repr√©senter efficacement les donn√©es."
    else:
        var_interpretation += "\nAucun coude clair n'est d√©tect√© dans le graphique d'√©boulis. Consid√©rez l'examen de la variance expliqu√©e cumul√©e pour d√©terminer combien de composantes conserver."
    
    # Interpr√©ter les contributions des caract√©ristiques
    feature_interpretation = "### Contributions des caract√©ristiques\n\n"
    
    # Interpr√©tation CP1
    pc1_loading = loadings[:, 0]
    sorted_idx = np.argsort(np.abs(pc1_loading))[::-1]
    top_features_pc1 = [(feature_names[idx], pc1_loading[idx]) for idx in sorted_idx[:3]]
    
    feature_interpretation += "**Composante Principale 1 (CP1)** est le plus fortement influenc√©e par :\n\n"
    for feature, loading in top_features_pc1:
        direction = "positivement" if loading > 0 else "n√©gativement"
        feature_interpretation += f"* {feature} (corr√©l√©e {direction}, coefficient = {loading:.3f})\n"
    
    # Interpr√©tation CP2
    pc2_loading = loadings[:, 1]
    sorted_idx = np.argsort(np.abs(pc2_loading))[::-1]
    top_features_pc2 = [(feature_names[idx], pc2_loading[idx]) for idx in sorted_idx[:3]]
    
    feature_interpretation += "\n**Composante Principale 2 (CP2)** est le plus fortement influenc√©e par :\n\n"
    for feature, loading in top_features_pc2:
        direction = "positivement" if loading > 0 else "n√©gativement"
        feature_interpretation += f"* {feature} (corr√©l√©e {direction}, coefficient = {loading:.3f})\n"
    
    # Recherche de clusters ou mod√®les dans le biplot
    cluster_interpretation = "### Analyse des motifs\n\n"
    
    # V√©rification des valeurs aberrantes
    distances = np.sqrt(df_pca[:, 0]**2 + df_pca[:, 1]**2)
    threshold = np.mean(distances) + 2 * np.std(distances)
    outliers = np.where(distances > threshold)[0]
    
    if len(outliers) > 0:
        cluster_interpretation += f"Il y a {len(outliers)} valeurs aberrantes potentielles d√©tect√©es dans le biplot, qui sont des observations √©loign√©es du centre du graphique.\n\n"
    
    # V√©rification des mod√®les de regroupement (approche tr√®s simple)
    from sklearn.cluster import KMeans
    
    # Essayer 2-4 clusters et v√©rifier le score silhouette
    from sklearn.metrics import silhouette_score
    
    best_score = -1
    best_n_clusters = 2
    
    if len(df_pca) > 10:  # Besoin d'un nombre raisonnable d'√©chantillons
        for n_clusters in range(2, min(5, len(df_pca) // 5 + 1)):
            try:
                kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                labels = kmeans.fit_predict(df_pca[:, :2])
                
                if len(np.unique(labels)) > 1:  # S'assurer d'avoir au moins 2 clusters
                    score = silhouette_score(df_pca[:, :2], labels)
                    
                    if score > best_score:
                        best_score = score
                        best_n_clusters = n_clusters
            except:
                pass
        
        if best_score > 0.3:
            cluster_interpretation += f"Les donn√©es semblent former approximativement {best_n_clusters} groupes dans l'espace CP1-CP2 (score silhouette = {best_score:.2f}), sugg√©rant des sous-groupes distincts dans vos donn√©es.\n\n"
        elif best_score > 0:
            cluster_interpretation += f"Il pourrait y avoir un certain regroupement dans les donn√©es, mais les clusters ne sont pas fortement s√©par√©s (score silhouette = {best_score:.2f}).\n\n"
        else:
            cluster_interpretation += "Aucun mod√®le de regroupement clair n'est √©vident dans l'espace ACP.\n\n"
    
    # R√©sum√© global
    summary = "### R√©sum√©\n\n"
    summary += f"La dimensionnalit√© de vos donn√©es peut √™tre r√©duite √† {elbow_component if elbow_detected else 2} composantes principales tout en conservant la plupart des informations importantes. "
    
    # Identifier les caract√©ristiques corr√©l√©es
    correlation_summary = ""
    for i in range(len(feature_names)):
        for j in range(i+1, len(feature_names)):
            if abs(np.dot(loadings[i], loadings[j])) > 0.7:
                correlation_summary += f"* {feature_names[i]} et {feature_names[j]} semblent √™tre fortement corr√©l√©s.\n"
    
    if correlation_summary:
        summary += "\n\nCaract√©ristiques corr√©l√©es identifi√©es :\n" + correlation_summary
    
    # Recommandations finales
    recommendations = "### Recommandations\n\n"
    recommendations += "Bas√©es sur les r√©sultats de l'ACP :\n\n"
    
    if explained_variance_ratio[0] > 0.5:
        recommendations += "* CP1 explique une majorit√© de la variance, sugg√©rant que les donn√©es pourraient √™tre efficacement repr√©sent√©es dans une dimension inf√©rieure.\n"
    
    if elbow_detected:
        recommendations += f"* Envisagez d'utiliser les {elbow_component} premi√®res composantes principales pour une analyse ou une mod√©lisation ult√©rieure.\n"
    else:
        cumulative_var = np.cumsum(explained_variance_ratio)
        for i, var in enumerate(cumulative_var):
            if var > 0.8:
                recommendations += f"* Pour conserver 80% de la variance, utilisez les {i+1} premi√®res composantes principales.\n"
                break
    
    if len(outliers) > 0:
        recommendations += "* Examinez les valeurs aberrantes identifi√©es dans le biplot car elles pourraient repr√©senter des anomalies importantes ou des probl√®mes de qualit√© des donn√©es.\n"
    
    # Combiner toutes les interpr√©tations
    full_interpretation = var_interpretation + "\n\n" + feature_interpretation + "\n\n" + cluster_interpretation + "\n\n" + summary + "\n\n" + recommendations
    
    return full_interpretation

# G√©n√©rer un lien t√©l√©chargeable pour le graphique
def get_plot_download_link(fig, filename="plot.png", text="T√©l√©charger le graphique"):
    buf = io.BytesIO()
    fig.savefig(buf, format='png', dpi=300, bbox_inches='tight')
    buf.seek(0)
    b64 = base64.b64encode(buf.read()).decode()
    href = f'<a href="data:image/png;base64,{b64}" download="{filename}">{text}</a>'
    return href

# Fonction principale d'analyse ACP
def run_pca_analysis(df, selected_columns, color_by, scale_data, n_components):
    """Ex√©cuter l'analyse ACP et g√©n√©rer des graphiques"""
    
    # Pr√©parer les donn√©es pour l'ACP
    X = df[selected_columns].copy()
    
    # G√©rer les valeurs manquantes (remplacer par la moyenne)
    X = X.fillna(X.mean())
    
    # Mettre √† l'√©chelle les donn√©es si demand√©
    if scale_data:
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
    else:
        X_scaled = X.values
    
    # Ex√©cuter l'ACP
    pca = PCA(n_components=min(n_components, len(selected_columns)))
    pc_scores = pca.fit_transform(X_scaled)
    
    # Cr√©er un DataFrame pour faciliter le tra√ßage
    pc_df = pd.DataFrame(data=pc_scores, columns=[f'CP{i+1}' for i in range(pc_scores.shape[1])])
    
    # Ajouter la colonne de couleur si sp√©cifi√©e
    if color_by != "Aucun":
        pc_df['color'] = df[color_by].values
    
    # Calculer les chargements (corr√©lations entre variables et composantes principales)
    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
    
    # Cr√©er des graphiques
    col1, col2 = st.columns(2)
    
    # 1. Graphique d'√©boulis
    with col1:
        st.header("Graphique d'√©boulis")
        fig1, ax1 = plt.subplots(figsize=(10, 6))
        
        # Diagramme √† barres de la variance expliqu√©e
        ax1.bar(range(1, len(pca.explained_variance_ratio_) + 1), 
                pca.explained_variance_ratio_, alpha=0.6, color='skyblue', 
                label='Variance expliqu√©e individuelle')
        
        # Graphique en ligne de la variance expliqu√©e cumul√©e
        ax1.step(range(1, len(pca.explained_variance_ratio_) + 1), 
                 np.cumsum(pca.explained_variance_ratio_), where='mid',
                 label='Variance expliqu√©e cumul√©e', color='red', marker='o')
        
        # Ajouter une ligne horizontale √† 80% et 95%
        ax1.axhline(y=0.8, color='g', linestyle='--', alpha=0.5, label='Seuil de variance 80%')
        ax1.axhline(y=0.95, color='purple', linestyle='--', alpha=0.5, label='Seuil de variance 95%')
        
        # Ajouter des √©tiquettes et une l√©gende
        ax1.set_xlabel('Composantes Principales')
        ax1.set_ylabel('Ratio de Variance Expliqu√©e')
        ax1.set_title('Graphique d\'√©boulis : Variance expliqu√©e par composante principale')
        ax1.set_xticks(range(1, len(pca.explained_variance_ratio_) + 1))
        ax1.set_xticklabels([f'CP{i}' for i in range(1, len(pca.explained_variance_ratio_) + 1)])
        ax1.legend(loc='best')
        ax1.grid(True, linestyle='--', alpha=0.5)
        
        # Afficher le graphique
        st.pyplot(fig1)
        
        # Tableau d√©taill√© de la variance
        var_df = pd.DataFrame({
            'Composante Principale': [f'CP{i+1}' for i in range(len(pca.explained_variance_ratio_))],
            'Valeur propre': pca.explained_variance_,
            'Variance expliqu√©e (%)': pca.explained_variance_ratio_ * 100,
            'Variance cumul√©e (%)': np.cumsum(pca.explained_variance_ratio_) * 100
        })
        
        st.subheader("Informations d√©taill√©es sur la variance")
        st.dataframe(var_df.style.format({
            'Valeur propre': '{:.3f}',
            'Variance expliqu√©e (%)': '{:.2f}%',
            'Variance cumul√©e (%)': '{:.2f}%'
        }))
        
        # Lien de t√©l√©chargement
        st.markdown(get_plot_download_link(fig1, "graphique_eboulis.png", "üì• T√©l√©charger le graphique d'√©boulis"), unsafe_allow_html=True)
    
    # 2. Biplot
    with col2:
        st.header("Biplot")
        fig2, ax2 = plt.subplots(figsize=(10, 8))
        
        # Configurer les couleurs
        if color_by != "Aucun":
            if pd.api.types.is_numeric_dtype(df[color_by]):
                scatter = ax2.scatter(pc_df['CP1'], pc_df['CP2'], c=pc_df['color'], 
                               cmap='viridis', alpha=0.7, s=50)
                plt.colorbar(scatter, ax=ax2, label=color_by)
            else:
                categories = df[color_by].unique()
                cmap = plt.get_cmap('tab10', len(categories))
                
                for i, category in enumerate(categories):
                    mask = pc_df['color'] == category
                    ax2.scatter(pc_df.loc[mask, 'CP1'], pc_df.loc[mask, 'CP2'], 
                              label=str(category), color=cmap(i), alpha=0.7, s=50)
                
                ax2.legend(title=color_by)
        else:
            ax2.scatter(pc_df['CP1'], pc_df['CP2'], alpha=0.7, s=50)
        
        # Tracer les vecteurs de caract√©ristiques
        for i, feature in enumerate(selected_columns):
            ax2.arrow(0, 0, loadings[i, 0] * 5, loadings[i, 1] * 5, 
                    head_width=0.2, head_length=0.2, fc='red', ec='red')
            ax2.text(loadings[i, 0] * 5.2, loadings[i, 1] * 5.2, feature, color='red')
        
        # Ajouter des √©tiquettes et un titre
        ax2.set_xlabel(f'Composante Principale 1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
        ax2.set_ylabel(f'Composante Principale 2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
        ax2.set_title('Biplot ACP : Projection des observations et des variables')
        
        # Ajouter des lignes de grille
        ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.3)
        ax2.axvline(x=0, color='gray', linestyle='--', alpha=0.3)
        ax2.grid(True, linestyle='--', alpha=0.3)
        
        # Rendre le graphique joli
        ax2.set_aspect('equal')
        
        # Afficher le graphique
        st.pyplot(fig2)
        
        # Lien de t√©l√©chargement
        st.markdown(get_plot_download_link(fig2, "biplot.png", "üì• T√©l√©charger le Biplot"), unsafe_allow_html=True)
    
    # Cr√©er un tableau de chargements
    loadings_df = pd.DataFrame(
        loadings, 
        columns=[f'CP{i+1}' for i in range(loadings.shape[1])],
        index=selected_columns
    )
    
    # Interpr√©tation IA
    st.header("Interpr√©tation des r√©sultats de l'ACP")
    
    with st.spinner("G√©n√©ration de l'interpr√©tation en cours..."):
        interpretation = interpret_pca_results(
            pca, 
            selected_columns, 
            loadings, 
            pca.explained_variance_ratio_,
            pc_scores
        )
    
    st.markdown(f'''
    <div class='interpretation'>
        {interpretation}
    </div>
    ''', unsafe_allow_html=True)
    
    # Afficher les chargements
    st.header("Contributions des variables")
    st.dataframe(loadings_df.style.background_gradient(cmap='coolwarm', axis=1))
    
    # Options de t√©l√©chargement
    st.header("Exporter les r√©sultats")
    
    # T√©l√©charger les scores
    scores_csv = pc_df.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="T√©l√©charger les scores CP (CSV)",
        data=scores_csv,
        file_name="scores_acp.csv",
        mime="text/csv"
    )
    
    # T√©l√©charger les chargements
    loadings_csv = loadings_df.to_csv().encode('utf-8')
    st.download_button(
        label="T√©l√©charger les contributions des variables (CSV)",
        data=loadings_csv,
        file_name="contributions_acp.csv",
        mime="text/csv"
    )

# Ex√©cuter l'analyse si les donn√©es sont t√©l√©charg√©es et les colonnes s√©lectionn√©es
if uploaded_file is not None and 'selected_columns' in locals() and selected_columns:
    if len(selected_columns) < 2:
        st.error("Veuillez s√©lectionner au moins 2 colonnes pour l'analyse ACP.")
    else:
        run_pca_analysis(df, selected_columns, color_by, scale_data, n_components)
else:
    # Afficher une sortie d'exemple si aucune donn√©e n'est t√©l√©charg√©e
    st.info("T√©l√©chargez un fichier CSV et s√©lectionnez des colonnes pour effectuer l'analyse ACP.")
    
    # Ajouter une option de jeu de donn√©es d'exemple
    if st.button("Charger un jeu de donn√©es d'exemple"):
        # Cr√©er un jeu de donn√©es d'exemple simple
        np.random.seed(42)
        n_samples = 100
        
        # Cr√©er quelques caract√©ristiques corr√©l√©es
        x1 = np.random.normal(0, 1, n_samples)
        x2 = x1 * 0.8 + np.random.normal(0, 0.2, n_samples)
        x3 = np.random.normal(0, 1, n_samples)
        x4 = x3 * 0.7 + np.random.normal(0, 0.3, n_samples)
        x5 = np.random.normal(0, 1, n_samples)
        
        # Cr√©er une variable cat√©gorielle pour la coloration
        group = np.concatenate([np.zeros(n_samples//2), np.ones(n_samples//2)])
        
        # Cr√©er un dataframe
        example_df = pd.DataFrame({
            'Variable1': x1,
            'Variable2': x2,
            'Variable3': x3,
            'Variable4': x4,
            'Variable5': x5,
            'Groupe': group
        })
        
        # Ex√©cuter l'ACP sur les donn√©es d'exemple
        run_pca_analysis(
            example_df, 
            ['Variable1', 'Variable2', 'Variable3', 'Variable4', 'Variable5'], 
            'Groupe', 
            True, 
            5
        )

# Ajouter un pied de page
st.markdown("""
---
### Comment utiliser cet outil :

1. **T√©l√©chargez un fichier CSV** contenant vos donn√©es.
2. **S√©lectionnez les variables** pour l'analyse ACP.
3. **Optionnel** : Choisissez une colonne pour coder en couleur les points dans le biplot.
4. **Examinez les r√©sultats** :
   - Le graphique d'√©boulis montre combien de variance chaque composante principale explique
   - Le Biplot montre comment les observations et les variables sont positionn√©es dans l'espace CP
   - L'interpr√©tation IA explique les principales conclusions de votre analyse ACP
5. **T√©l√©chargez** les graphiques et les donn√©es pour vos rapports et analyses suppl√©mentaires.

L'ACP est une technique de r√©duction de dimensionnalit√© qui transforme des variables corr√©l√©es en un ensemble plus petit de variables non corr√©l√©es appel√©es composantes principales. Cela aide √† visualiser des mod√®les dans des donn√©es √† haute dimension.
""")